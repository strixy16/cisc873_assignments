{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta7NLtZKSTpe"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50ujX_1D81bE"
      },
      "outputs": [],
      "source": [
        "# if you haven't installed xgboost on your system, uncomment the line below\n",
        "!pip install xgboost\n",
        "# if you haven't installed bayesian-optimization on your system, uncomment the line below\n",
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUTqFAlW8pqB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.svm import SVC\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from xgboost.sklearn import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wf9KobwSWOI"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIGpFPqh8vLZ"
      },
      "outputs": [],
      "source": [
        "# Load train and test data into pandas dataframe\n",
        "data = pd.read_csv('train.csv')\n",
        "data_test = pd.read_csv('test.csv')\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DzU1vhz8y4h"
      },
      "outputs": [],
      "source": [
        "# Plot histogram of how many attributes have a certain number of null values\n",
        "data.isnull().sum().hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3BqmY2P80R0"
      },
      "outputs": [],
      "source": [
        "# Plot distribution of values in match attribute\n",
        "data['match'].hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWVKzKIi82_1"
      },
      "outputs": [],
      "source": [
        "# Removing match (this is our target attribute) and id from training set\n",
        "x = data.drop(['match', 'id'], axis=1)\n",
        "# Create list of numeric features by checking if they contain float type\n",
        "features_numeric = list(x.select_dtypes(include=['float64']))\n",
        "# Create list of categorical features by check if they contain object type\n",
        "features_categorical = list(x.select_dtypes(include=['object']))\n",
        "# Setting labels to be match column from train data\n",
        "y = data['match']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgn0X4_V9RL9"
      },
      "source": [
        "# Preprocessing Pipeline Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA-I1WRx9KUF"
      },
      "outputs": [],
      "source": [
        "# Preprocessing steps for pipeline\n",
        "\n",
        "# Transformations for numeric data\n",
        "# Impute missing values with median value in feature by default\n",
        "# Standardize featuers by removing the mean and scaling to unit variance\n",
        "transformer_numeric = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())]\n",
        ")\n",
        "\n",
        "# Transformations for categorical data\n",
        "# Impute missing value with 'missing' by default\n",
        "# Encode features using one-hot numeric array, ignoring unknown categorical \n",
        "# features\n",
        "transformer_categorical = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Apply corresponding transformers to numeric and categorical features in data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', transformer_numeric, features_numeric),\n",
        "        ('cat', transformer_categorical, features_categorical)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCXhNGht9lNK"
      },
      "source": [
        "# Model Parameters and Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkWrlYIt8g7J"
      },
      "source": [
        "## XGBoost\n",
        "### Parameters:\n",
        "Didn't know defaults for this, so based choices off of Random Forest Classifier.\n",
        "\n",
        "\n",
        "*    **N_estimators**: used default 100 from RFC, let the range cover lower than the default and greater to see if either direction was better\n",
        "*    **max_depth**: RFC default 'None', wanted to see if limiting this impacted the model, maybe preventing overfitting\n",
        "*    **learning**: RFC default 0.0001, always talked about to train in papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2nIA1X59n1q"
      },
      "outputs": [],
      "source": [
        "# XGBboost pipeline\n",
        "XGB_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('xgb_classifier', XGBClassifier(\n",
        "            objective='binary:logistic', seed=1))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Parameter grid for grid and random search\n",
        "XGB_param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
        "    'preprocessor__cat__imputer__strategy': ['constant', 'most_frequent'],\n",
        "    'xgb_classifier__n_estimators': [50, 100, 500],\n",
        "    'xgb_classifier__max_depth': ['None', 50, 100],\n",
        "    'xgb_classifier__learning': [0.001, 0.0001, 0.00001]\n",
        "}\n",
        "\n",
        "# Parameter grid for Bayes search centered on defaults\n",
        "XGB_bayes_grid1 = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean', 'median']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['constant', 'most_frequent']),\n",
        "    'xgb_classifier__n_estimators': Integer(50, 500),\n",
        "    'xgb_classifier__max_depth': Integer(50, 100),\n",
        "    'xgb_classifier__learning': Real(1e-5, 1e-3, prior='log-uniform')\n",
        "}\n",
        "\n",
        "# Parameter grid that focuses on results from default grid \n",
        "XGB_bayes_grid2 = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['constant']),\n",
        "    'xgb_classifier__n_estimators': Integer(70, 100),\n",
        "    'xgb_classifier__max_depth': Integer(65, 85),\n",
        "    'xgb_classifier__learning': Real(1e-6, 1e-5, prior='log-uniform')\n",
        "}\n",
        "\n",
        "# Set model as XGB\n",
        "model_type = 'XGB'\n",
        "pipeline = XGB_pipeline\n",
        "param_grid = XGB_param_grid\n",
        "bayes_grid = XGB_bayes_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-S7ojmJ_vHJ"
      },
      "source": [
        "## Support Vector Machine\n",
        "### Parameters:\n",
        "*    **C**: default 1.0, regularization parameter\n",
        "*    **kernel**: default rbf, wanted to try different kernels on the problme\n",
        "*    **max_iter**: default -1, seeing if early stopping made a difference on result. Got convergence warnings so removed this argument. \n",
        "*    **degree**: default 3, added in second Bayes model since polynomial kernel was chosen as best option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpSF00PzDJmV"
      },
      "outputs": [],
      "source": [
        "# Support Vector Machine model\n",
        "# Pipeline\n",
        "SVM_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('svm_classifier', SVC(probability=True, class_weight='balanced'))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Parameter grid for grid and random search\n",
        "SVM_param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
        "    'preprocessor__cat__imputer__strategy': ['constant', 'most_frequent'],\n",
        "    'svm_classifier__C': [0.5, 1.0, 1.5],\n",
        "    'svm_classifier__kernel': ['linear', 'poly', 'rbf'],\n",
        "    # 'svm_classifier__max_iter': [-1, 500, 1000]\n",
        "}\n",
        "\n",
        "# Parameter grid for Bayes search centered on defaults\n",
        "SVM_bayes_grid = {\n",
        "    # Numeric imputation with mean or median\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean', 'median']),\n",
        "    # Categorical imputation with constant \"missing\" value, or use most frequent\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['constant', 'most_frequent']),\n",
        "    'svm_classifier__C': Real(0.5, 1.5, prior='log-uniform'),\n",
        "    'svm_classifier__kernel': Categorical(['linear', 'poly', 'rbf']),\n",
        "    # 'svm_classifier__max_iter': Integer(-1, 1000)\n",
        "}\n",
        "\n",
        "# Parameter grid that focuses on results from default grid \n",
        "SVM_bayes_grid2 = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean', 'median']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['most_frequent']),\n",
        "    'svm_classifier__C': Real(1.0, 3.0, prior='log-uniform'),\n",
        "    'svm_classifier__kernel': Categorical(['poly']),\n",
        "    'svm_classifier__degree': Integer(2, 4)\n",
        "}\n",
        "\n",
        "# Set model as SVM\n",
        "model_type = 'SVM'\n",
        "pipeline = SVM_pipeline\n",
        "param_grid = SVM_param_grid\n",
        "bayes_grid = SVM_bayes_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNTNTS7GAfzB"
      },
      "source": [
        "## Multi-Layer Perceptron\n",
        "### Parameters:\n",
        "*    **hidden_layer_sizes**: had this for grid search but couldn't figure out how to use it in Bayes so eliminated it\n",
        "*    **alpha**: default 0.0001, learning rate always mentioned to train in papers and in class\n",
        "*    **max_iter**: default 200, seeing if stopping earlier or running longer is better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eYqCz5UGvwS"
      },
      "outputs": [],
      "source": [
        "# Multi-Layer Perceptron model\n",
        "# Pipeline\n",
        "MLP_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('mlp_classifier', MLPClassifier())\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Parameter grid for grid and random seach\n",
        "MLP_param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
        "    'preprocessor__cat__imputer__strategy': ['constant', 'most_frequent'],\n",
        "    'mlp_classifier__hidden_layer_sizes': [(100,)],\n",
        "    'mlp_classifier__alpha':[0.001, 0.0001, 0.00001],\n",
        "    'mlp_classifier__max_iter':[100, 200, 400]\n",
        "}\n",
        "\n",
        "# Parameter grid for Bayes search centered on defaults\n",
        "MLP_bayes_grid = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean', 'median']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['constant', 'most_frequent']),\n",
        "    'mlp_classifier__alpha': Real(1e-5, 1e-3, prior='log-uniform'),\n",
        "    'mlp_classifier__max_iter': Integer(100, 400)\n",
        "}\n",
        "\n",
        "# Parameter grid that focuses on results from default grid \n",
        "MLP_bayes_grid2 = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['most_frequent']),\n",
        "    'mlp_classifier__alpha': Real(1e-5, 1e-4, prior='log-uniform'),\n",
        "    'mlp_classifier__max_iter': Integer(320, 380)\n",
        "}\n",
        "\n",
        "# Set model as MLP\n",
        "model_type = 'MLP'\n",
        "pipeline = MLP_pipeline\n",
        "param_grid = MLP_param_grid\n",
        "bayes_grid = MLP_bayes_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIIy1OOOBnBE"
      },
      "source": [
        "## Random Forest Classifier\n",
        "### Parameters:\n",
        "*     **n_estimators**: default 100, seeing if more or less trees in the forest makes a difference\n",
        "*     **criterion**: default gini, only two options here so adding it wouldn't grow grid too much"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk-gjaCWqgDD"
      },
      "outputs": [],
      "source": [
        "# Random Forest Classifier model\n",
        "# Pipeline\n",
        "RFC_pipeline = Pipeline(\n",
        "    steps=[\n",
        "           ('preprocessor', preprocessor),\n",
        "           ('rfc_classifier', RandomForestClassifier())\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Parameter grid for grid and random seach\n",
        "RFC_param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
        "    'preprocessor__cat__imputer__strategy': ['constant', 'most_frequent'],\n",
        "    'rfc_classifier__n_estimators': [50, 100, 500],\n",
        "    'rfc_classifier__max_depth': ['None', 50, 100],\n",
        "    'rfc_classifier__criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Parameter grid for Bayes search that centers on defaults\n",
        "RFC_bayes_grid = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean', 'median']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['constant', 'most_frequent']),\n",
        "    'rfc_classifier__n_estimators': Integer(50, 500),\n",
        "    'rfc_classifier__criterion': Categorical(['gini', 'entropy'])\n",
        "}\n",
        "\n",
        "# Parameter grid that focuses on results from default grid \n",
        "RFC_bayes_grid2 = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['most_frequent']),\n",
        "    'rfc_classifier__n_estimators': Integer(380, 440),\n",
        "    'rfc_classifier__criterion': Categorical(['entropy'])\n",
        "}\n",
        "\n",
        "# Set model as RFC\n",
        "model_type = 'RFC'\n",
        "pipeline = RFC_pipeline\n",
        "param_grid = RFC_param_grid\n",
        "bayes_grid = RFC_bayes_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiUDcy6HCg_j"
      },
      "source": [
        "## Logistic Regression\n",
        "### Parameters:\n",
        "*     **C**: default 1.0, regularization term, included to compare to SVM\n",
        "*     **max_iter**: default 100, as with SVM and MLP seeing if stopping earlier or later makes any difference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWzXWPsysiUY"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression Model\n",
        "# Pipeline\n",
        "LG_pipeline = Pipeline(\n",
        "    steps=[\n",
        "           ('preprocessor', preprocessor),\n",
        "           ('lg_classifier', LogisticRegression())\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Parameter grid for grid and random seach\n",
        "LG_param_grid = {\n",
        "    'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
        "    'preprocessor__cat__imputer__strategy': ['constant', 'most_frequent'],\n",
        "    'lg_classifier__C': [0.5, 1.0, 1.5],\n",
        "    'lg_classifier__max_iter':[50, 100, 200]\n",
        "}\n",
        "\n",
        "# Parameter grid for Bayes search that centers on defaults\n",
        "LG_bayes_grid = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean', 'median']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['constant', 'most_frequent']),\n",
        "    'lg_classifier__C': Real(0.5, 1.5, prior='log-uniform'),\n",
        "    'lg_classifier__max_iter': Integer(100, 500)\n",
        "}\n",
        "\n",
        "# First parameter grid didn't use enough iterations (had Convergence Warnings)\n",
        "LG_bayes_grid2 = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean', 'median']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['constant', 'most_frequent']),\n",
        "    'lg_classifier__C': Real(0.5, 1.5, prior='log-uniform'),\n",
        "    'lg_classifier__max_iter': Integer(1000,5000)\n",
        "}\n",
        "\n",
        "# Parameter grid that focuses on results from grid2\n",
        "LG_bayes_grid3 = {\n",
        "    'preprocessor__num__imputer__strategy': Categorical(['mean']),\n",
        "    'preprocessor__cat__imputer__strategy': Categorical(['constant']),\n",
        "    'lg_classifier__C': Real(0.4, 0.6, prior='log-uniform'),\n",
        "    'lg_classifier__max_iter': Integer(1160,1220)\n",
        "}\n",
        "\n",
        "# Set model as LG\n",
        "model_type = 'LG'\n",
        "pipeline = LG_pipeline\n",
        "param_grid = LG_param_grid\n",
        "bayes_grid = LG_bayes_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDbgdT5_9_cQ"
      },
      "source": [
        "# Parameter Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GhA5w5BR9kjr"
      },
      "outputs": [],
      "source": [
        "# Grid Search\n",
        "# Generate grid search to optimize the hyperparameters\n",
        "# Estimator is classifier specified in pipeline\n",
        "# cv is number of folds to use in cross-validation\n",
        "# verbose indicates level of messages to print\n",
        "# n_jobs is number of jobs to run in parallel\n",
        "# scoring uses area under the receiver operating characteristic curve\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, verbose=3, n_jobs=2, \n",
        "    scoring='roc_auc')\n",
        "\n",
        "# Run fit with all sets of hyperparameters on the training data\n",
        "grid_search.fit(x, y)\n",
        "\n",
        "print('best score {}'.format(grid_search.best_score_))\n",
        "print('best parameters {}'.format(grid_search.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IfKPKVe9cgS"
      },
      "outputs": [],
      "source": [
        "# Random Search\n",
        "# Estimator is classifier specified in pipeline\n",
        "# cv is number of folds to use in cross-validation\n",
        "# verbose indicates level of messages to print\n",
        "# n_jobs is number of jobs to run in parallel\n",
        "# n_iter is number of random parameter sets to choose\n",
        "# scoring uses area under the receiver operating characteristic curve\n",
        "random_search = RandomizedSearchCV(\n",
        "    pipeline, param_grid, cv=5, verbose=3, n_jobs=2, n_iter=10,\n",
        "    scoring='roc_auc'\n",
        ")\n",
        "\n",
        "# Run fit with the randomly chosen hyperparameters on the training data\n",
        "random_search.fit(x,y)\n",
        "\n",
        "print('best score {}'.format(random_search.best_score_))\n",
        "print('best parameters {}'.format(random_search.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNm1U4cY_ZaT"
      },
      "outputs": [],
      "source": [
        "# Bayes Search\n",
        "bayes_search = BayesSearchCV(pipeline, bayes_grid, n_iter=10, cv=5, verbose=3)\n",
        "\n",
        "# Fits n_iter samples from parameter settings to the training data\n",
        "bayes_search.fit(x, y)\n",
        "\n",
        "print('best score {}'.format(bayes_search.best_score_))\n",
        "print('best score {}'.format(bayes_search.best_params_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcHoMlqQ82l"
      },
      "source": [
        "# Output Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcHijR6VR1Nf"
      },
      "outputs": [],
      "source": [
        "# NOTE: Only run this cell once. If data_test changes, need to run loading cell again before this one\n",
        "# Prepare submission:\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = data_test['id']\n",
        "# Drop ID column\n",
        "data_test.drop(columns='id', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LuJtsl7K3wrK"
      },
      "outputs": [],
      "source": [
        "# Grid search fit submission csv\n",
        "submission['match'] = grid_search.predict_proba(data_test)[:,1]\n",
        "filename = model_type + '_grid_submission.csv'\n",
        "submission.to_csv(filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_yL78e28xdz"
      },
      "outputs": [],
      "source": [
        "# Random search fit submission csv\n",
        "submission['match'] = random_search.predict_proba(data_test)[:,1]\n",
        "filename = model_type + '_random_submission.csv'\n",
        "submission.to_csv(filename, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYb4Kgl03tTw"
      },
      "outputs": [],
      "source": [
        "# Bayes search fit submission csv\n",
        "submission_bayes['match'] = bayes_search.predict_proba(data_test)[:,1]\n",
        "filename = model_type + '_bayes_submission2.csv'\n",
        "submission_bayes.to_csv(filename, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Code.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
