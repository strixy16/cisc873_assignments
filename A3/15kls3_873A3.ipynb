{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code",
      "provenance": [],
      "collapsed_sections": [
        "KbVaJTHvBFz0",
        "hhseEaACQZE2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1TrUxOFLy5t"
      },
      "source": [
        "import collections\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNGRU\n",
        "from tensorflow.keras.layers import GRU, LSTM, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbK3G9JIBASM"
      },
      "source": [
        "# Data Loading and Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBDf95jL7se"
      },
      "source": [
        "# Load in train and test data to pandas dataframe\n",
        "xy_train_df = pd.read_csv('xy_train.csv')\n",
        "x_test_df = pd.read_csv('x_test.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr8rvZRMav6U"
      },
      "source": [
        "# Remove incorrectly labelled data\n",
        "xy_train_df.drop(xy_train_df[xy_train_df.label > 1].index, inplace=True)\n",
        "xy_train_df.reset_index(inplace=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwV1ZlKpiP0f",
        "outputId": "54909901-fb95-43e7-de19-9105621ae7fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "xy_train_df.shape\n",
        "# Should be (59768, 4)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59768, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUbFFPk3BCKA"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuv018_1FAGt"
      },
      "source": [
        "# Separate out the training data and labels\n",
        "x = xy_train_df.text\n",
        "y = xy_train_df.label"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFDGgmfABx_8"
      },
      "source": [
        "# Remove English stopwords\n",
        "def removeStopEnglish(list_of_text):\n",
        "    # Get list of English words to remove\n",
        "    stop = stopwords.words('english')\n",
        "    # Set all input text to lowercase for string comparison\n",
        "    list_of_text = list_of_text.str.lower()\n",
        "    # Removing all English stopwords from input text \n",
        "    # (Source for this lambda function in report)\n",
        "    list_of_text_nostop = list_of_text.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "    # Return updated list\n",
        "    return list_of_text_nostop\n",
        "\n",
        "x = removeStopEnglish(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHz4KgLUphA3"
      },
      "source": [
        "x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnelLvdeOWSY"
      },
      "source": [
        "# Maximum number of words to keep based on word frequency (variable for Tokenizer)\n",
        "vocab_size = 40000\n",
        "\n",
        "# Build vocabulary from training set\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "# Preprocessing function\n",
        "def padText(list_of_text, max_len):\n",
        "    # Transforms list of sequences into 2D numpy array\n",
        "    # Sequences shorter than maxlen are padded at the end ('post' argument)\n",
        "    # Sequences longer than maxlen are truncated \n",
        "    return pad_sequences(\n",
        "        tokenizer.texts_to_sequences(list_of_text),\n",
        "        maxlen=max_len,\n",
        "        padding='post',\n",
        "    )\n",
        "# Length to make all sequences in vocabulary (variable for pad_sequences)\n",
        "max_len = 40  \n",
        "# padding is done inside: \n",
        "x = padText(x, max_len)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tF4Y11GkqmV",
        "outputId": "0f88e55f-3b51-450c-ec6d-8aa68f7e1b8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Split the training data into train and validate, 20% validation at beginning\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_valid.shape, y_valid.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(47814, 40) (47814,)\n",
            "(11954, 40) (11954,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X7uP76tSVWM",
        "outputId": "89020d24-dd22-4c33-e546-60fc13634391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Display last 5 entries with pretty printer\n",
        "pprint(tokenizer.sequences_to_texts(x_train[:5]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['pregnant escaped las vegas concert shooting couple welcomed baby world',\n",
            " 'church polish tourist place full inside people wait outside listening '\n",
            " 'preacher',\n",
            " \"scott confirms negan's kill premiere confirms premiere show killed we've \"\n",
            " 'learned nothing new',\n",
            " 'burrs outback central australia size lifesaver affectionately known teddy '\n",
            " \"bear's\",\n",
            " '25 000 pounds honey creates sticky situation new zealand terror attack '\n",
            " 'denied bail']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueWFSE5RSZo9",
        "outputId": "c1b78d5c-95c7-44c7-905d-15e2536a07b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('total words in the dictionary', tokenizer.num_words)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total words in the dictionary 40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk2I_nZIBDuG"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbVaJTHvBFz0"
      },
      "source": [
        "## Fully Connected Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgsoeFfiSgAZ"
      },
      "source": [
        "# MODEL 1\n",
        "# Version 1: No stopword removal\n",
        "# Version 2: English stopwords removed\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "# Reduce embedded back to 2D by taking the mean along axis 1 \n",
        "averaged = tf.reduce_mean(embedded, axis=1) # [None, 100]\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(averaged) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='functional_1_1'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ3CI_-Qm_5f"
      },
      "source": [
        "# MODEL 2\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "# Reduce embedded back to 2D by taking the mean along axis 1 \n",
        "averaged = tf.reduce_mean(embedded, axis=1) # [None, 100]\n",
        "# Fully connected layer\n",
        "x = keras.layers.Dense(16)(averaged) # [None, 16]\n",
        "x = keras.layers.Dropout(0.5)(x) # [None, 16]\n",
        "x = keras.layers.Dense(8)(x) # [None, 8]\n",
        "x = keras.layers.Dropout(0.5)(x) # [None, 8]\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='fully_connected_2_2'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhseEaACQZE2"
      },
      "source": [
        "## Recurrent GRU Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MB2wHN_xrTjz"
      },
      "source": [
        "Simple Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOUGOuhuQqKb"
      },
      "source": [
        "# GRU model 1\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = GRU(75)(embedded) # [None, 75]\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='gru_1'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-T6_50lrwO3"
      },
      "source": [
        "# GRU simple model 2\n",
        "# Single layer, smaller number of units\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = GRU(4)(embedded) # [None, 4]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='gru_simple_2'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,\n",
        "                    callbacks=keras.callbacks.EarlyStopping())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OeJ90wbrVim"
      },
      "source": [
        "Multi-layer GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q29lflDqTCfX"
      },
      "source": [
        "# GRU model 2 - Multi layer\n",
        "# Added second GRU layer\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = GRU(75, return_sequences=True)(embedded) # [None, 100, 75]\n",
        "x = GRU(75)(x) # [None, 75]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='gru_2'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF8Swl_NrbSj"
      },
      "source": [
        "Multi-Layer GRU #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o-G-6neWzdE"
      },
      "source": [
        "# GRU model 3 - Multi Layer\n",
        "# Changed layer unit sizes\n",
        "# Adding callbacks EarlyStopping in fit\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = GRU(16, return_sequences=True)(embedded) # [None, 100, 75]\n",
        "x = GRU(8)(x) # [None, 75]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='gru_3'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,\n",
        "                    callbacks=keras.callbacks.EarlyStopping())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnqgBYK5re3_"
      },
      "source": [
        "Bidirectional GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umnKTHinrhIN",
        "outputId": "da14da12-4311-4d24-b135-ef86d12f5aa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# GRU Model 4 - Bidirectional, single layer \n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = Bidirectional(GRU(4))(embedded) # [None, 8]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='gru_bidirectional_1'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,\n",
        "                    callbacks=keras.callbacks.EarlyStopping())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"gru_bidirectional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 40)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 40, 100)           4000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 8)                 2544      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 4,002,553\n",
            "Trainable params: 4,002,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIb2mOtu399J"
      },
      "source": [
        "# GRU Model 4 - Bidirectional, multi-layer \n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = Bidirectional(GRU(4))(embedded) # [None, 8]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='gru_bidirectional_1'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,\n",
        "                    callbacks=keras.callbacks.EarlyStopping())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTJhGNhUbZlR"
      },
      "source": [
        "## LSTM Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s22Kek_46Gx"
      },
      "source": [
        "Simple Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fIhQHM3cIbB"
      },
      "source": [
        "# LSTM Model 1\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = LSTM(4)(embedded) # [None, 4]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='lstm_1'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,\n",
        "                    callbacks=keras.callbacks.EarlyStopping())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0leEwaN48Py"
      },
      "source": [
        "Multi-layer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzRiIyG7dlwh"
      },
      "source": [
        "# LSTM Model 2 - Multi-layer\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = LSTM(32, return_sequences=True)(embedded) # [None, max_len, 32]\n",
        "x = LSTM(16, return_sequences=True)(x) # [None, max_len, 16]\n",
        "x = LSTM(8, return_sequences=True)(x) # [None, max_len, 8]\n",
        "x = LSTM(4)(x) # [None, 4]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='lstm_2'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,\n",
        "                    callbacks=keras.callbacks.EarlyStopping())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeSbpVPW4_QE"
      },
      "source": [
        "Simple Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDpPOsXukaTg"
      },
      "source": [
        "# LSTM Model 3\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = LSTM(4)(embedded) # [None, 4]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='lstm_3'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,\n",
        "                    callbacks=keras.callbacks.EarlyStopping())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCpG_Mj_5CWw"
      },
      "source": [
        "Bidirectional Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZRR9I574swR"
      },
      "source": [
        "# LSTM Bidirectional Model 1 \n",
        "# No early stopping, single bidirectional layer\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = Bidirectional(LSTM(4))(embedded) # [None, 8]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='lstm_bidirectional_1'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCM1P4nRjbSI"
      },
      "source": [
        "# LSTM Bidirectional Model 2 \n",
        "# Same as model 1, using decreased learning rate\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = Bidirectional(LSTM(4))(embedded) # [None, 8]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='lstm_bidirectional_2'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 15\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,)\n",
        "                    # callbacks=keras.callbacks.EarlyStopping())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRhcnJwZqQVd"
      },
      "source": [
        "# LSTM Bidirectional Multilayer Model\n",
        "# No early stopping, multi bidirectional layer\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = Bidirectional(LSTM(8, return_sequences=True))(embedded) # [None, max_len, 16]\n",
        "x = Bidirectional(LSTM(4))(x) # [None, 8]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='lstm_bidirectional_3'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMOmAfjZygso"
      },
      "source": [
        "# LSTM Model 1\n",
        "# Input layer for neural network\n",
        "seq_in = keras.Input(batch_shape=(None, max_len)) # [None, max_len]\n",
        "# Layer that turns positive integers into dense vectors of fixed size\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 200)(seq_in) # [None, max_len, 100]\n",
        "\n",
        "x = LSTM(8)(embedded) # [None, 4]\n",
        "\n",
        "# Output layer, sigmoid activation\n",
        "pred = keras.layers.Dense(1, activation='sigmoid')(x) # [None, 1]\n",
        "\n",
        "# Group layers together into model\n",
        "model = keras.Model(\n",
        "    inputs=seq_in,\n",
        "    outputs=pred,\n",
        "    name='lstm_1_3'\n",
        ")\n",
        "\n",
        "# Configure model for training\n",
        "model.compile(\n",
        "    # Optimize using Adam \n",
        "    optimizer=Adam(),\n",
        "    # Value to be minimized by the model\n",
        "    loss='binary_crossentropy',\n",
        "    # Evaluate model with these metrics (Kaggle using AUC)\n",
        "    metrics=['accuracy', 'AUC']\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "batch_size = 64\n",
        "# Fit model to training data\n",
        "history = model.fit(x_train,\n",
        "                    y_train,\n",
        "                    # Number of iterations over entire x and y data\n",
        "                    epochs=epochs,\n",
        "                    # Number of samples per gradient update\n",
        "                    batch_size=batch_size,\n",
        "                    # Data to test model on\n",
        "                    validation_data=(x_valid, y_valid),\n",
        "                    # Shows progress bar for each epoch\n",
        "                    verbose=1,)\n",
        "                    # callbacks=keras.callbacks.EarlyStopping())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flDq-LxOBNt_"
      },
      "source": [
        "# Model Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jgg10EKG3EYp"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO5kAqMu39Hv"
      },
      "source": [
        "# Create AUC metric vs. epoch plot\n",
        "plt.plot(history.history['auc'])\n",
        "plt.plot(history.history['val_auc'])\n",
        "plt.title('model accuracy - ' + model.name)\n",
        "plt.ylabel('AUC')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WMU1ZzzBJee"
      },
      "source": [
        "# Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0DMDtODUlrD"
      },
      "source": [
        "# Remove English stop words from test set\n",
        "x_test = removeStopEnglish(x_test_df.text)\n",
        "# Process test set with padding/truncating to maxlen\n",
        "x_test = padText(x_test, max_len)\n",
        "# Run test set through model\n",
        "y_predict = np.squeeze(model.predict(x_test))\n",
        "\n",
        "# Setting up string for output file name\n",
        "output_name = model.name + '_' + str(epochs) + \"_\" + str(batch_size) + '.csv'\n",
        "\n",
        "# Put ID back together with prediction and save out for submission\n",
        "pd.DataFrame(\n",
        "    {'id': x_test_df.index,\n",
        "     'label': y_predict}).to_csv(output_name, index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}